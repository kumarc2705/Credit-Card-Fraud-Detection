# -*- coding: utf-8 -*-
"""IML_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tC7komaEA6Ca811Boh5JGxdQwBJizvf6
"""

import pandas as pd
import numpy as np
import os
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler,normalize
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve,make_scorer
from sklearn.model_selection import cross_validate

#score = logisticRegr.score(x_test, y_test)
#print(score)
#fraud_acc = len(real_fraud)/(len(real_fraud) + false_neg)
#print(fraud_acc)
# pred_fraud = np.where(predictions == 1)[0]
# real_fraud = np.where(y_test == 1)[0]
# false_pos = len(np.setdiff1d(pred_fraud, real_fraud))
# pred_good = np.where(predictions == 0)[0]
# real_good = np.where(y_test == 0)[0]
# false_neg = len(np.setdiff1d(pred_good, real_good))
# false_neg_rate = false_neg/(false_pos+false_neg)
# accuracy = (len(x_test) - (false_neg + false_pos)) / len(x_test)
# fraud_acc = len(real_fraud)/(len(real_fraud) + false_neg)
# print("Number of true positive cases: "+ str(len(real_good)))
# print("Number of false positive cases: "+ str(false_pos))
# print("Number of true negative cases: "+ str(len(real_fraud)))
# print("Number of false negative cases: "+ str(false_neg))
# print("Overall accuracy in Logistic Regression is: "+ str(accuracy))
# print("Accuracy for fraud detection in Logistic Regression is: "+ str(fraud_acc))

from google.colab import drive
# drive.mount('/content/drive', force_remount=True)

# df=pd.read_csv('/content/drive/MyDrive/ML Final Project 412 /creditcard.csv')
#print(df)  
df.isna().sum()

plt.hist(df['Amount'], bins=60, log=True)
plt.xlabel('Transaction amount')
plt.ylabel('Number of Transactions')
plt.title('Amount Distribution')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("ggplot")
sns.FacetGrid(df, hue="Class", height = 6).map(plt.scatter, "Time", "Amount", edgecolor="k").add_legend()
plt.ylabel('Transaction amount')
plt.xlabel('Time (s)')
plt.show()

plt.scatter(df['V1'],df['Class'], c='red')
plt.xlabel('V1')
plt.ylabel('Class')
plt.title('V1 Vs Class')
plt.show()

plt.scatter(df['V2'],df['Class'], c='green')
plt.xlabel('V2')
plt.ylabel('Class')
plt.title('V2 Vs Class')
plt.show()

plt.scatter(df['V3'],df['Class'], c='blue')
plt.xlabel('V3')
plt.ylabel('Class')
plt.title('V3 Vs Class')
plt.show()



plt.scatter(df['V4'],df['Class'], c='purple')
plt.xlabel('V4')
plt.ylabel('Class')
plt.title('V4 Vs Class')
plt.show()

x_data = df.drop(['Time', 'Class'], axis = 1)
scaler = RobustScaler()
x_data = scaler.fit_transform(x_data)
print(x_data.shape)
y_data = df['Class']
print(y_data.shape)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=0)

x_train = np.array(x_train)
x_train_norm = normalize(x_train)
x_test = np.array(x_test)
x_test_norm = normalize(x_test)
y_train = np.array(y_train)
y_test = np.array(y_test)
X = np.concatenate((x_train,x_test),axis=0)
Y = np.concatenate((y_train,y_test),axis=0)
X_Norm = np.concatenate((x_train_norm,x_test_norm),axis=0)
Y_Norm = np.concatenate((y_train,y_test),axis=0)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import LogisticRegression
logisticRegr = LogisticRegression(max_iter = 4000, C=1e5)
logisticRegr.fit(x_train, y_train)
predictions = logisticRegr.predict(x_test)
accuracy=accuracy_score(y_test,predictions)
precison=precision_score(y_test,predictions)
recall=recall_score(y_test,predictions)
f1=f1_score(y_test,predictions)
print("-------Logistic Regression------Without Normalization-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)
scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(logisticRegr, X_Norm,
                         Y_Norm, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------K-Fold Cross Validation for Logistic Regression-----------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

from sklearn.linear_model import LogisticRegression
logisticRegr = LogisticRegression(max_iter = 4000, C=1e5)
logisticRegr.fit(x_train_norm, y_train)
predictions = logisticRegr.predict(x_test_norm)
accuracy=accuracy_score(y_test,predictions)
precison=precision_score(y_test,predictions)
recall=recall_score(y_test,predictions)
f1=f1_score(y_test,predictions)
print("-------Logistic Regression------With Normalization-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)
scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(logisticRegr, X,
                         Y, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------K-Fold Cross Validation for Logistic Regression-----------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

#K-means Without Normalization
from sklearn.cluster import KMeans
kmeans = KMeans(init='k-means++', n_clusters=2, n_init=1,max_iter = 20000)
kmeans.fit(x_train)

predictions = kmeans.predict(x_test)
tn,fp,fn,tp=confusion_matrix(y_test,predictions).ravel()
#scoring knn

accuracy=accuracy_score(y_test,predictions)
precison=precision_score(y_test,predictions)
recall=recall_score(y_test,predictions)
f1=f1_score(y_test,predictions)
print("-------KMeans----Without Normalization---")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)
scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(kmeans, X,
                         Y, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------K-Fold Cross Validation for Logistic Regression----Without Normaliation-------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

#K-means With Normalization
from sklearn.cluster import KMeans
kmeans = KMeans(init='k-means++', n_clusters=2, n_init=1,max_iter = 20000)
kmeans.fit(x_train_norm)

predictions = kmeans.predict(x_test_norm)
tn,fp,fn,tp=confusion_matrix(y_test,predictions).ravel()
#scoring knn

accuracy=accuracy_score(y_test,predictions)
precison=precision_score(y_test,predictions)
recall=recall_score(y_test,predictions)
f1=f1_score(y_test,predictions)
print("-------KMeans----With Normalization---")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)
scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(kmeans, X_Norm,
                         Y_Norm, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------K-Fold Cross Validation for KMeans----With Normaliation-------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

#SVM Without Normalization
from sklearn import svm
clf = svm.SVC(kernel='linear',max_iter = 1000)
clf.fit(x_train,y_train)

predictions = clf.predict(x_test)
accuracy=accuracy_score(y_test,predictions)
precison=precision_score(y_test,predictions)
recall=recall_score(y_test,predictions)
f1=f1_score(y_test,predictions)
print("-------SVM-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)
scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(clf,X ,
                         Y, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------K-Fold Cross Validation for SVM----Without Normaliation-------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

#SVM Without Normalization
from sklearn import svm
clf = svm.SVC(kernel='linear',max_iter = 1000)
clf.fit(x_train_norm,y_train)

predictions = clf.predict(x_test_norm)
accuracy=accuracy_score(y_test,predictions)
precison=precision_score(y_test,predictions)
recall=recall_score(y_test,predictions)
f1=f1_score(y_test,predictions)
print("-------SVM-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)
scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(clf,X_Norm ,
                         Y_Norm, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------K-Fold Cross Validation for SVM----With Normaliation-------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

# Neural Network Model without Normalization
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

batch_size=10000
# Function to create model
def create_model():
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=29, activation='relu'))
	model.add(Dense(8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model

# create model
model = create_model()
#model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=1,
                    validation_split=0.1)

predictions = np.array(model.predict(x_test))
predictions = np.where(predictions >=0.5, float(1), predictions)
predictions = np.where(predictions <0.5, float(0), predictions)

score = model.evaluate(x_test, y_test,
                       batch_size=batch_size, verbose=1)
print('Test accuracy:', score[1])
pred_fraud = len(np.where(predictions == 1)[0])
real_fraud = len(np.where(y_test == 1)[0])
false_pos = len(np.setdiff1d(pred_fraud, real_fraud))
pred_good = len(np.where(predictions == 0)[0])
real_good = len(np.where(y_test == 0)[0])
false_neg = len(np.setdiff1d(pred_good, real_good))
#false_neg_rate = false_neg/(false_pos+false_neg)
accuracy = (len(x_test) - (false_neg + false_pos)) / len(x_test)
fraud_acc = real_fraud/(real_fraud + false_neg)
# print("Number of true positive cases: "+ str(len(real_fraud)))
# print("Number of false positive cases: "+ str(false_pos))
# print("Number of true negative cases: "+ str(len(real_good)))
# print("Number of false negative cases: "+ str(false_neg))
# print("Overall accuracy in Neural 3 layer Network Model is: "+ str(accuracy))
accuracy=accuracy_score(y_test,predictions)
print("-------Neural Network without Normalization-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
precison=  tp/(tp +fp)
recall=tp/(tp + fn)
f1 = precison*recall/(precison + recall)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)

# Neural Network Model with Normalization
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score

batch_size=10000
# Function to create model
def create_model():
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=29, activation='relu'))
	model.add(Dense(8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model

# create model
model = create_model()
#model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)
history = model.fit(x_train_norm, y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=1,
                    validation_split=0.1)

predictions = np.array(model.predict(x_test_norm))
predictions = np.where(predictions >=0.5, float(1), predictions)
predictions = np.where(predictions <0.5, float(0), predictions)

accuracy=accuracy_score(y_test,predictions)
print("-------Neural Network with Normalization-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
precison=  tp/(tp +fp)
recall=tp/(tp + fn)
f1 = precison*recall/(precison + recall)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)

###!! KNN ##
import pandas as pd
import numpy as np
import time
from matplotlib import pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D 
plt.style.use('ggplot')
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve
#from sklearn.externals import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import os
from sklearn import metrics
from sklearn.preprocessing import RobustScaler

from google.colab import drive
# drive.mount('/content/drive', force_remount=True)

# df=pd.read_csv('/content/drive/MyDrive/ML Final Project 412 /creditcard.csv')

# Standardize The Features.
df['SAmount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))
#df['STime'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))

x_data = df.drop(['Time', 'Class', 'Amount'], axis = 1)

# To Check For Missing Values!
df.isnull().sum()

#X = df.drop(['Class'], axis = 1)
#y = df['Class']

##pca = PCA(n_components=2)
#principalComponents = pca.fit_transform(X.values)
#principalDf = pd.DataFrame(data = principalComponents
            # , columns = ['principal component 1', 'principal component 2'])

#finalDf = pd.concat([principalDf, y], axis = 1)
#finalDf.head()

## New Method
tic=time.time()
# df=pd.read_csv('/content/drive/MyDrive/ML Final Project 412 /creditcard.csv')
df=df.sample(frac=1)#randomize the whole dataset
x=df.drop(["Time","Class"],axis=1) #X
y=pd.DataFrame(df[["Class"]])  #Y
x_array=x.values
y_array=y.values
x_train,x_test,y_train,y_test=train_test_split(x_array,y_array,train_size=0.8)

print("KNN Without NORMALIZAtion!!")

knn=KNeighborsClassifier(n_neighbors=5)   #,algorithm="kd_tree",n_jobs=-1
knn.fit(x_train,y_train.ravel())
kn_predicted=knn.predict(x_test)

tn,fp,fn,tp=confusion_matrix(y_test,kn_predicted).ravel()
#scoring knn

accuracy=accuracy_score(y_test,kn_predicted)
precison=precision_score(y_test,kn_predicted)
recall=recall_score(y_test,kn_predicted)
f1=f1_score(y_test,kn_predicted)

print("-------KNN-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)

#print("Accuracy for Fraud Transcations :-")
#temp = tn + fp
#faccu = tn/(temp)
#print(faccu)

#CROsss

#scoring = {'accuracy' : make_scorer(accuracy_score), 
          # 'precision' : make_scorer(precision_score),
          # 'recall' : make_scorer(recall_score), 
           #'f1_score' : make_scorer(f1_score)}

#cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
#scores = cross_validate(knn, X_Norm,
#                         Y_Norm, scoring=scoring, cv=cv, n_jobs=-1)
##print("-----------------K-Fold Cross Validation for Logistic Regression-----------")
#print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
#print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
#print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
#print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

print("KNN WITH NORMALIZATION")

knn.fit(x_train_norm,y_train.ravel())
kn_predicted=knn.predict(x_test_norm)

tn,fp,fn,tp=confusion_matrix(y_test,kn_predicted).ravel()
#scoring knn

accuracy=accuracy_score(y_test,kn_predicted)
precison=precision_score(y_test,kn_predicted)
recall=recall_score(y_test,kn_predicted)
f1=f1_score(y_test,kn_predicted)

print("-------KNN-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)

##------------------------- End OF KNN



### Gaussian Naive Bayes

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#df=pd.read_csv('/content/drive/MyDrive/412 Project/creditcard.csv')
#df=df.sample(frac=1)#randomize the whole dataset
#X=df.drop(["Time","Class"],axis=1) #X
#y#=pd.DataFrame(df[["Class"]])  #Y
#X_array=X.values
#y_array=y.values


#X_train, X_test, y_train, y_test = train_test_split(x_array, y_array, test_size = 0.3, random_state = 0)

X_train, X_test, y_train, y_test = train_test_split(x_array, y_array, train_size = 0.8, random_state = 0)


# instantiate the model
gnb = GaussianNB()

gnb.fit(X_train,y_train.ravel())

# fit the model
#gm.fit(X_train, y_train.ravel())

#y_train.ravel().shape

##FIt

#gnb.fit(X_train, y_train.ravel())

y_pred = gnb.predict(X_test)

#print(y_pred.shape)

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

Yt = y_test.ravel()

#y_pred.shape

#X = X.reshape(X.shape[1:])
accuracy_score(Yt, y_pred)

cm = confusion_matrix(y_test, y_pred)

tn,fp,fn,tp=confusion_matrix(y_test,y_pred).ravel()

accuracy=accuracy_score(y_test,y_pred)
precison=precision_score(y_test,y_pred)
recall=recall_score(y_test,y_pred)
f1=f1_score(y_test,y_pred)

print("-------Gaussian NB-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
precison=  tp/(tp +fp)
recall=tp/(tp + fn)
f1 = precison*recall/(precison + recall)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)

#CROsss

scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_validate(gnb, X_Norm, Y_Norm, scoring=scoring, cv=cv, n_jobs=-1)
print("-----------------Gaussian Validation for Gaussian N.Bayes-----------")
print("K-Fold Accuracy: "+str(np.mean(scores['test_accuracy'])))
print("K-Fold Precision: "+str(np.mean(scores['test_precision'])))
print("K-Fold Recall: "+str(np.mean(scores['test_recall'])))
print("K-Fold F1 Score: "+str(np.mean(scores['test_f1_score'])))

print("With Normalization")

gnb.fit(x_train_norm,y_train.ravel())

y_pred = gnb.predict(x_test_norm)

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))



accuracy_score(y_test.ravel(), y_pred)

cm = confusion_matrix(y_test, y_pred)

tn,fp,fn,tp=confusion_matrix(y_test,y_pred).ravel()

accuracy=accuracy_score(y_test,y_pred)
precison=precision_score(y_test,y_pred)
recall=recall_score(y_test,y_pred)
f1=f1_score(y_test,y_pred)

print("-------Gaussian NB-------")
print("Confusion Matrix : ")
print("\t tn =",tn,"\t fp =",fp)
print("\t fn =",fn,"\t tp =",tp)
print("\n Accuracy :",accuracy)
precison=  tp/(tp +fp)
recall=tp/(tp + fn)
f1 = precison*recall/(precison + recall)
print("\n Precison :",precison)
print("\n Recall :",recall)
print("\n F1 :",f1)



## -------- Gaussian NB End

